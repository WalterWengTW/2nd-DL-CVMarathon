{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 『本次練習內容』\n",
    "#### 運用這幾天所學觀念搭建一個CNN分類器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 『本次練習目的』\n",
    "  #### 熟悉CNN分類器搭建步驟與原理\n",
    "  #### 學員們可以嘗試不同搭法，如使用不同的Maxpooling層，用GlobalAveragePooling取代Flatten等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import ReLU\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print(x_train.shape) #(50000, 32, 32, 3)\n",
    "\n",
    "## Normalize Data\n",
    "def normalize(X_train,X_test):\n",
    "        mean = np.mean(X_train,axis=(0,1,2,3))\n",
    "        std = np.std(X_train, axis=(0, 1, 2, 3))\n",
    "        X_train = (X_train-mean)/(std+1e-7)\n",
    "        X_test = (X_test-mean)/(std+1e-7) \n",
    "        return X_train, X_test,mean,std\n",
    "    \n",
    "    \n",
    "## Normalize Training and Testset    \n",
    "x_train, x_test,mean_train,std_train = normalize(x_train, x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "## OneHot Label 由(None, 1)-(None, 10)\n",
    "## ex. label=2,變成[0,0,1,0,0,0,0,0,0,0]\n",
    "one_hot=OneHotEncoder()\n",
    "y_train=one_hot.fit_transform(y_train).toarray()\n",
    "y_test=one_hot.transform(y_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0309 14:58:22.848359  7196 deprecation_wrapper.py:119] From C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0309 14:58:22.865290  7196 deprecation_wrapper.py:119] From C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0309 14:58:22.867285  7196 deprecation_wrapper.py:119] From C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0309 14:58:22.895237  7196 deprecation_wrapper.py:119] From C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0309 14:58:22.896231  7196 deprecation_wrapper.py:119] From C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0309 14:58:25.660873  7196 deprecation_wrapper.py:119] From C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0309 14:58:25.724725  7196 deprecation_wrapper.py:119] From C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\ipykernel_launcher.py:52: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=10)`\n",
      "W0309 14:58:26.375990  7196 deprecation_wrapper.py:119] From C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0309 14:58:26.492675  7196 deprecation.py:323] From C:\\Users\\walter\\Anaconda3\\envs\\DLTest\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,911,050\n",
      "Trainable params: 1,908,426\n",
      "Non-trainable params: 2,624\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 16s 311us/step - loss: 1.2059 - acc: 0.5639\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 13s 260us/step - loss: 0.7925 - acc: 0.72141s - loss: 0.8093  - ETA: 1s - loss: 0.8\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 13s 258us/step - loss: 0.6353 - acc: 0.7782\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.5389 - acc: 0.8133\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 0.4556 - acc: 0.8419\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 0.3901 - acc: 0.8622\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.3332 - acc: 0.8829\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 13s 254us/step - loss: 0.2779 - acc: 0.9021\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 13s 253us/step - loss: 0.2398 - acc: 0.9167\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 13s 259us/step - loss: 0.1977 - acc: 0.9305\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 13s 255us/step - loss: 0.1676 - acc: 0.9403\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 13s 254us/step - loss: 0.1394 - acc: 0.9509\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 13s 254us/step - loss: 0.1237 - acc: 0.9562\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 13s 255us/step - loss: 0.1046 - acc: 0.9633\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 13s 258us/step - loss: 0.0946 - acc: 0.96711s - \n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 13s 258us/step - loss: 0.0873 - acc: 0.9695\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 13s 259us/step - loss: 0.0797 - acc: 0.9713\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 12s 250us/step - loss: 0.0706 - acc: 0.9748\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 0.0642 - acc: 0.9763\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 13s 253us/step - loss: 0.0643 - acc: 0.9778\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.0571 - acc: 0.9796\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 13s 254us/step - loss: 0.0595 - acc: 0.9792\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 13s 258us/step - loss: 0.0512 - acc: 0.9821\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 13s 251us/step - loss: 0.0514 - acc: 0.9815\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 0.0493 - acc: 0.9826\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 0.0444 - acc: 0.9848\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 0.0430 - acc: 0.9848\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 0.0442 - acc: 0.9839\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 0.0355 - acc: 0.9876\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 0.0441 - acc: 0.9845\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 13s 253us/step - loss: 0.0359 - acc: 0.9878\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 13s 258us/step - loss: 0.0370 - acc: 0.9872\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 12s 250us/step - loss: 0.0365 - acc: 0.9871\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 14s 270us/step - loss: 0.0307 - acc: 0.9901\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 13s 256us/step - loss: 0.0358 - acc: 0.9880\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 13s 256us/step - loss: 0.0303 - acc: 0.9896\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.0324 - acc: 0.9881\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 13s 264us/step - loss: 0.0312 - acc: 0.9892\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 13s 252us/step - loss: 0.0221 - acc: 0.9927\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 13s 258us/step - loss: 0.0314 - acc: 0.9890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 13s 269us/step - loss: 0.0303 - acc: 0.9896\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 13s 270us/step - loss: 0.0276 - acc: 0.9903\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 14s 278us/step - loss: 0.0261 - acc: 0.9910\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 13s 263us/step - loss: 0.0257 - acc: 0.9913\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.0247 - acc: 0.9918\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 0.0259 - acc: 0.9911\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.0242 - acc: 0.9915\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 13s 256us/step - loss: 0.0184 - acc: 0.9936\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 14s 275us/step - loss: 0.0244 - acc: 0.9917\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 13s 258us/step - loss: 0.0274 - acc: 0.9906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2385dc90668>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier=Sequential()\n",
    "\n",
    "#卷積組合\n",
    "classifier.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', input_shape=(32,32,3)))#32,3,3,input_shape=(32,32,3),activation='relu''\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(ReLU())\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "\n",
    "'''自己決定MaxPooling2D放在哪裡'''\n",
    "#classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#卷積組合\n",
    "classifier.add(Conv2D(filters=64, kernel_size=(3,3), padding='same'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(ReLU())\n",
    "\n",
    "classifier.add(Conv2D(filters=64, kernel_size=(3,3), padding='same'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(ReLU())\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "classifier.add(Conv2D(filters=128, kernel_size=(3,3), padding='same'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(ReLU())\n",
    "\n",
    "classifier.add(Conv2D(filters=128, kernel_size=(3,3), padding='same'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(ReLU())\n",
    "\n",
    "classifier.add(Conv2D(filters=128, kernel_size=(3,3), padding='same'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(ReLU())\n",
    "\n",
    "classifier.add(Conv2D(filters=256, kernel_size=(3,3), padding='same'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(ReLU())\n",
    "\n",
    "classifier.add(Conv2D(filters=512, kernel_size=(3,3), padding='same'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(ReLU())\n",
    "\n",
    "#flatten\n",
    "#classifier.add(Flatten())\n",
    "classifier.add(GlobalAveragePooling2D())\n",
    "\n",
    "#FC\n",
    "#classifier.add(Dense(units= 1024, activation='relu')) #output_dim=100,activation=relu\n",
    "\n",
    "#輸出\n",
    "classifier.add(Dense(output_dim=10,activation='softmax'))\n",
    "classifier.summary()\n",
    "\n",
    "#超過兩個就要選categorical_crossentrophy\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "classifier.fit(x_train,y_train,batch_size=64,epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 預測新圖片，輸入影像前處理要與訓練時相同\n",
    "#### ((X-mean)/(std+1e-7) ):這裡的mean跟std是訓練集的\n",
    "## 維度如下方示範"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 228us/step\n",
      "Loss :     0.955679\n",
      "Accuracy : 0.833300\n"
     ]
    }
   ],
   "source": [
    "# input_example=(np.zeros(shape=(1,32,32,3))-mean_train)/(std_train+1e-7) \n",
    "# classifier.predict(input_example)\n",
    "\n",
    "loss, acc = classifier.evaluate(x_test, y_test)\n",
    "print(\"Loss :     %f\"%loss)\n",
    "print(\"Accuracy : %f\"%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
